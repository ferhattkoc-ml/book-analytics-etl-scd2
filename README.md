Book Analytics ETL Pipeline (SCD Type 2)

Production-style ETL pipeline built with Python, Pandas and MySQL, implementing full data validation, transformation, historical tracking (SCD Type 2) and run-level logging.

â¸»

ğŸš€ Project Purpose

This project simulates a real-world analytics ETL pipeline.

The main goals were:
	â€¢	âœ… Design a relational database model
	â€¢	âœ… Clean and normalize raw CSV data
	â€¢	âœ… Build a modular ETL architecture
	â€¢	âœ… Implement Slowly Changing Dimension (Type 2)
	â€¢	âœ… Add production-grade logging and monitoring
	â€¢	âœ… Follow real data engineering best practices

â¸»

ğŸ— Database Design

The schema follows a simplified star-schema-like structure.

ğŸ“Œ Dimension Tables
	â€¢	kitap_adlari
	â€¢	yazar_adlari
	â€¢	kitap_turleri
	â€¢	dil

Each dimension table contains:
	â€¢	PRIMARY KEY
	â€¢	Descriptive attributes

â¸»

ğŸ“Œ Fact Table

fact_table includes:
	â€¢	fact_id (AUTO_INCREMENT, surrogate key)
	â€¢	kitap_id (FK)
	â€¢	yazar_id (FK)
	â€¢	kitap_tur_id (FK)
	â€¢	dil_id (FK)
	â€¢	yayin_tarihi
	â€¢	satis_adedi
	â€¢	satis_tutari

Each row represents:

1 book + 1 publication date + sales metrics

ğŸ” Constraints Enforced
	â€¢	PRIMARY KEY on dimensions
	â€¢	FOREIGN KEY constraints
	â€¢	DECIMAL(10,2) for monetary values
	â€¢	ISO date format (YYYY-MM-DD)

â¸»

ğŸ§¹ Data Cleaning & Normalization

Raw CSV data initially contained:
	â€¢	âŒ Turkish date format (15.08.2019)
	â€¢	âŒ Thousand separators (2.207)
	â€¢	âŒ Decimal commas (30.765,58)

ğŸ”„ Transformations Applied
	â€¢	Dates converted to ISO format
	â€¢	Thousand separators removed
	â€¢	Decimal commas converted to dot notation
	â€¢	Numeric columns cast to correct data types
	â€¢	Monetary values standardized

This ensured consistent and clean ingestion into MySQL.

